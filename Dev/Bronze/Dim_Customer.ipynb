{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c9af90e-b232-4cd0-98c1-cfb3d71db863",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade dbldatagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3484fda-461d-46f8-817b-efea06c0ae8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# CONFIG\n",
    "# ---------------------------------------------\n",
    "spark.sql(\"USE CATALOG dev\")\n",
    "db_name = \"dev.bronze\"\n",
    "table_name = \"dim_customer\"\n",
    "full_table_name = f\"{db_name}.{table_name}\"\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Imports\n",
    "# ---------------------------------------------\n",
    "import dbldatagen as dg\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Read current max surrogate key (if table exists)\n",
    "# ---------------------------------------------\n",
    "exists = spark.catalog.tableExists(full_table_name)\n",
    "if exists:\n",
    "    max_sk_val = (\n",
    "        spark.table(full_table_name)\n",
    "        .agg(F.max(\"customer_sk\").alias(\"max_sk\"))\n",
    "        .collect()[0][\"max_sk\"]\n",
    "    )\n",
    "    max_sk = int(max_sk_val) if max_sk_val is not None else 0\n",
    "else:\n",
    "    max_sk = 0\n",
    "import dbldatagen as dg\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "num_new_rows = 50\n",
    "partitions = 8\n",
    "email_domain = \"example.com\"\n",
    "\n",
    "# City → State mapping\n",
    "city_to_state = {\n",
    "    \"Hyderabad\": \"Telangana\",\n",
    "    \"Mumbai\": \"Maharashtra\",\n",
    "    \"Delhi\": \"Delhi\",\n",
    "    \"Bangalore\": \"Karnataka\",\n",
    "    \"Chennai\": \"Tamil Nadu\",\n",
    "    \"Pune\": \"Maharashtra\",\n",
    "    \"Visakhapatnam\": \"Andhra Pradesh\",\n",
    "    \"Kolkata\": \"West Bengal\",\n",
    "    \"Ahmedabad\": \"Gujarat\",\n",
    "    \"Jaipur\": \"Rajasthan\",\n",
    "}\n",
    "\n",
    "first_names = [\n",
    "    \"Anita\",\n",
    "    \"Priya\",\n",
    "    \"Sneha\",\n",
    "    \"Neha\",\n",
    "    \"Pooja\",\n",
    "    \"Kavya\",\n",
    "    \"Arjun\",\n",
    "    \"Amit\",\n",
    "    \"Rahul\",\n",
    "    \"Vikram\",\n",
    "    \"Sanjay\",\n",
    "    \"Varun\",\n",
    "    \"Kiran\",\n",
    "]\n",
    "last_names = [\n",
    "    \"Sharma\",\n",
    "    \"Singh\",\n",
    "    \"Patel\",\n",
    "    \"Gupta\",\n",
    "    \"Iyer\",\n",
    "    \"Reddy\",\n",
    "    \"Khan\",\n",
    "    \"Das\",\n",
    "    \"Nair\",\n",
    "    \"Joshi\",\n",
    "    \"Chowdhury\",\n",
    "    \"Gowda\",\n",
    "]\n",
    "segments = [\"Retail\", \"Wholesale\", \"VIP\"]\n",
    "\n",
    "# Build Spark SQL map() literal for city→state\n",
    "map_items = []\n",
    "for c, s in city_to_state.items():\n",
    "    map_items.extend([f\"'{c}'\", f\"'{s}'\"])\n",
    "map_expr = \"map(\" + \",\".join(map_items) + \")\"\n",
    "\n",
    "# Datagen spec\n",
    "dg_spec = (\n",
    "    dg.DataGenerator(spark, rows=num_new_rows, partitions=partitions)\n",
    "    .withIdOutput()\n",
    "    .withColumn(\"customer_sk\", \"long\", expr=f\"id + {max_sk} + 1\")\n",
    "    .withColumn(\"customer_id_num\", \"int\", minValue=1, maxValue=10000, random=False)\n",
    "    .withColumn(\"first_name\", \"string\", values=first_names, random=True)\n",
    "    .withColumn(\"last_name\", \"string\", values=last_names, random=True)\n",
    "    .withColumn(\"city\", \"string\", values=list(city_to_state.keys()), random=True)\n",
    "    .withColumn(\"state\", \"string\", expr=f\"element_at({map_expr}, city)\")\n",
    "    .withColumn(\"country\", \"string\", values=[\"India\"], random=True)\n",
    "    .withColumn(\"customer_segment\", \"string\", values=segments, random=True)\n",
    "    .withColumn(\"effective_from\", \"date\", expr=\"current_date()\")\n",
    "    .withColumn(\"effective_to\", \"date\", expr=\"'2099-12-31'\")\n",
    ")\n",
    "\n",
    "df_new = dg_spec.build()\n",
    "\n",
    "# Compose fields (prefix customer_id_num -> string \"CUST-<num>\")\n",
    "df_new = (\n",
    "    df_new.withColumn(\n",
    "        \"customer_id\", F.concat(F.lit(\"CUST-\"), F.col(\"customer_id_num\").cast(\"string\"))\n",
    "    )\n",
    "    .withColumn(\"Name\", F.concat_ws(\" \", F.col(\"first_name\"), F.col(\"last_name\")))\n",
    "    .withColumn(\n",
    "        \"email\",\n",
    "        F.concat(\n",
    "            F.lower(F.col(\"first_name\")),\n",
    "            F.lit(\".\"),\n",
    "            F.lower(F.col(\"last_name\")),\n",
    "            F.lit(\"@\"),\n",
    "            F.lit(email_domain),\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "# is_current = latest effective_from per *customer_id* (SCD2 rule)\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window.partitionBy(\"customer_id\").orderBy(F.col(\"effective_from\").desc())\n",
    "df_new = (\n",
    "    df_new.withColumn(\"rn_desc\", F.row_number().over(w))\n",
    "    .withColumn(\"is_current\", (F.col(\"rn_desc\") == 1))\n",
    "    .drop(\"rn_desc\", \"customer_id_num\")\n",
    ")\n",
    "\n",
    "display(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c02d54d-0f8e-4591-8f48-92bc8b396b99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"use catalog dev\")\n",
    "df_new.write.format(\"delta\").mode(\"append\").saveAsTable(\"bronze.dim_customer\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8703685717708364,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Dim_Customer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
